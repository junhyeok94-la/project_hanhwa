import streamlit as st
import pandas as pd
from sqlalchemy import create_engine
import psycopg2
import urllib.parse
import time

# ---
# âš ï¸ Pandas Styler ì„¤ì •: ê²½ê³  í•´ê²°
# DataFrameì˜ ì…€ ìˆ˜ê°€ ë§ì•„ì§ˆ ê²½ìš° Stylerê°€ ë Œë”ë§í•  ìˆ˜ ìˆë„ë¡ ìµœëŒ€ ì…€ ê°œìˆ˜ ì„¤ì •
pd.set_option("styler.render.max_elements", 1_000_000)
# ---

st.set_page_config(layout="wide")
st.title("DB ì •í•©ì„± ëª¨ë‹ˆí„°ë§")

# --------------------
# 1. secrets.tomlì—ì„œ Redshift ì ‘ì† ì •ë³´ ë¡œë“œ
# --------------------
try:
    redshift_secrets = st.secrets["redshift"]
except KeyError:
    st.error("`.streamlit/secrets.toml` íŒŒì¼ì— 'redshift' ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ì ‘ì† ì •ë³´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# --------------------
# 2. ë°ì´í„° ì¡°íšŒ í•¨ìˆ˜
# --------------------
@st.cache_data(ttl=3600, show_spinner=False)
def get_table_info(conn_config, schemas):
    """
    ì£¼ì–´ì§„ ìŠ¤í‚¤ë§ˆ ëª©ë¡ì˜ ëª¨ë“  í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ ì •ë³´ë¥¼ information_schema.columnsì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    with st.spinner(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° {schemas} ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë”© ì¤‘..."):
        try:
            conn_red = psycopg2.connect(
                dbname=conn_config["database"],
                host=conn_config["host"],
                port=conn_config["port"],
                user=conn_config["user"],
                password=conn_config["password"]
            )
            
            schema_list_str = ", ".join([f"'{s}'" for s in schemas])
            
            query = f"""
                SELECT
                    c.table_schema as tbl_schema,
                    c.table_name,
                    c.column_name,
                    pgd.description AS column_comment,
                    c.data_type,
                    c.character_maximum_length,
                    c.numeric_precision,
                    c.numeric_precision_radix
                FROM
                    information_schema.columns c
                LEFT JOIN
                    pg_catalog.pg_class t ON t.relname = c.table_name AND t.relnamespace = (SELECT oid FROM pg_catalog.pg_namespace WHERE nspname = c.table_schema)
                LEFT JOIN
                    pg_catalog.pg_attribute a ON a.attrelid = t.oid AND a.attname = c.column_name
                LEFT JOIN
                    pg_catalog.pg_description pgd ON pgd.objoid = t.oid AND pgd.objsubid = a.attnum
                WHERE
                    c.table_schema IN ({schema_list_str})
                    AND c.table_name NOT LIKE '%2025%'
                    AND c.table_name NOT LIKE 'drop%'
                    AND c.table_name NOT LIKE 'previous%'
                ORDER BY
                    c.table_schema, c.table_name, c.ordinal_position;
            """
            
            df = pd.read_sql(query, conn_red)
            conn_red.close()
            return df
        
        except Exception as e:
            st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë˜ëŠ” ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()

# --------------------
# 3. ì •í•©ì„± ì²´í¬ ë° ì‹œê°í™” í•¨ìˆ˜
# --------------------
def check_consistency(src_tables_df, tgt_tables_df, src_schema_name, tgt_schema_name):
    # 'base%'ë¡œ ì‹œì‘í•˜ëŠ” ìŠ¤í‚¤ë§ˆëª…ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬
    if src_schema_name == "ìš´ì˜ BASE":
        src_tables_df['table_schema'] = src_tables_df['tbl_schema'].apply(
            lambda x: x.replace('base_', '')
        )
    else:
        src_tables_df['table_schema'] = src_tables_df['tbl_schema']
    
    if tgt_schema_name == "ìš´ì˜ TOBE":
        tgt_tables_df['table_schema'] = tgt_tables_df['tbl_schema']
    
    # ğŸ’¡ ì •í•©ì„± ì²´í¬ ì„¤ëª… ì¶”ê°€
    st.markdown("""
    ì´ ê²€ì‚¬ëŠ” **SRCì™€ TGT ìŠ¤í‚¤ë§ˆ ê°„ì˜ ë©”íƒ€ë°ì´í„° ë¶ˆì¼ì¹˜**ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    - **SRC ì™€ TGT ê¸°ì¤€**
      - `SRC`: ê°œë°œ TOBE redshiftì˜ 'sdhub_own','dhub_own','gsbi_own' ìŠ¤í‚¤ë§ˆ
      - `TGT`: ìš´ì˜ TOBE redshiftì˜ 'sdhub_own','dhub_own','gsbi_own' ìŠ¤í‚¤ë§ˆ
    - **ì •í•©ì„± ë¶ˆì¼ì¹˜ ì²´í¬ ê¸°ì¤€:**
      - `'[TGT]'ì— ì»¬ëŸ¼ ì—†ìŒ`: SRCì—ë§Œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼
      - `'[SRC]'ì— ì»¬ëŸ¼ ì—†ìŒ`: TGTì—ë§Œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼
      - `ë°ì´í„° íƒ€ì… ë¶ˆì¼ì¹˜`: SRCì™€ TGTì˜ ë°ì´í„° íƒ€ì…ì´ ë‹¤ë¥¸ ê²½ìš°
      - `ì»¬ëŸ¼ ê¸¸ì´ ë¶ˆì¼ì¹˜`: VARCHAR, CHAR íƒ€ì…ì˜ ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²½ìš°
      - `ìˆ«ì íƒ€ì… ì •ë°€ë„ ë¶ˆì¼ì¹˜`: NUMERIC íƒ€ì…ì˜ ì •ë°€ë„ê°€ ë‹¤ë¥¸ ê²½ìš°
    """)

    merged_df = pd.merge(
        src_tables_df,
        tgt_tables_df,
        on=['table_schema', 'table_name', 'column_name'],
        how='outer',
        suffixes=('_src', '_tgt'),
        indicator=True
    )
    
    # ğŸ’¡ character_maximum_lengthë¥¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜ (ì†Œìˆ˜ì  ì œê±°)
    merged_df['character_maximum_length_src'] = pd.to_numeric(merged_df['character_maximum_length_src'], errors='coerce').astype('Int64')
    merged_df['character_maximum_length_tgt'] = pd.to_numeric(merged_df['character_maximum_length_tgt'], errors='coerce').astype('Int64')

    # ğŸ’¡ Status_detail ì»¬ëŸ¼ì„ ìœ„í•œ í•¨ìˆ˜
    def get_status_detail(row):
        if row['_merge'] == 'left_only':
            return f"'{tgt_schema_name}'ì— ì»¬ëŸ¼ ì—†ìŒ"
        if row['_merge'] == 'right_only':
            return f"'{src_schema_name}'ì— ì»¬ëŸ¼ ì—†ìŒ"

        if row['data_type_src'] != row['data_type_tgt']:
            return "ë°ì´í„° íƒ€ì… ë¶ˆì¼ì¹˜"

        if row['data_type_src'] in ['character varying', 'varchar', 'char']:
            if not pd.isna(row['character_maximum_length_src']) and not pd.isna(row['character_maximum_length_tgt']):
                if row['character_maximum_length_src'] != row['character_maximum_length_tgt']:
                    return "ì»¬ëŸ¼ ê¸¸ì´ ë¶ˆì¼ì¹˜"
            elif not pd.isna(row['character_maximum_length_src']) or not pd.isna(row['character_maximum_length_tgt']):
                return "ì»¬ëŸ¼ ê¸¸ì´ ë¶ˆì¼ì¹˜"

        if row['data_type_src'] == 'numeric':
            if not pd.isna(row['numeric_precision_src']) and not pd.isna(row['numeric_precision_tgt']):
                if row['numeric_precision_src'] != row['numeric_precision_tgt'] or \
                   row['numeric_precision_radix_src'] != row['numeric_precision_radix_tgt']:
                    return "ìˆ«ì íƒ€ì… ì •ë°€ë„ ë¶ˆì¼ì¹˜"
            elif not pd.isna(row['numeric_precision_src']) or not pd.isna(row['numeric_precision_tgt']):
                return "ìˆ«ì íƒ€ì… ì •ë°€ë„ ë¶ˆì¼ì¹˜"
        return "OK"

    # ğŸ’¡ Mismatch ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ê°„ë‹¨í•œ Status ì»¬ëŸ¼
    def get_status(detail_status):
        return "OK" if detail_status == "OK" else "Mismatch"

    merged_df['Status_detail'] = merged_df.apply(get_status_detail, axis=1)
    merged_df['Status'] = merged_df['Status_detail'].apply(get_status)

    total_count = len(merged_df)
    error_count = len(merged_df[merged_df['Status'] != 'OK'])

    # ğŸ’¡ í…Œì´ë¸” ë‹¨ìœ„ ì •í•©ì„± ì§€í‘œ ê³„ì‚°
    table_status_df = merged_df.groupby(['table_schema', 'table_name']).agg(
        total_columns=('column_name', 'size'),
        mismatch_columns=('Status', lambda x: (x == 'Mismatch').sum())
    ).reset_index()

    table_status_df['matching_rate'] = (1 - table_status_df['mismatch_columns'] / table_status_df['total_columns']).fillna(1) * 100
    
    table_status_df['table_status'] = table_status_df.apply(
        lambda row: 'OK' if row['mismatch_columns'] == 0 else 'Mismatch', axis=1
    )

    # ğŸ’¡ í…Œì´ë¸”, ì»¬ëŸ¼ ë‹¨ìœ„ ì§€í‘œ ì‹œê°í™”
    st.markdown("#### ì •í•©ì„± ìš”ì•½")
    
    col1, col2, col3 = st.columns(3)
    col1.metric("ì´ í…Œì´ë¸” ìˆ˜", len(table_status_df))
    col2.metric("ì •í•©ì„± ë¶ˆì¼ì¹˜ í…Œì´ë¸” ìˆ˜", len(table_status_df[table_status_df['table_status'] == 'Mismatch']))
    col3.metric("ì •í•©ì„± ì¼ì¹˜ìœ¨(í…Œì´ë¸” ê¸°ì¤€)", f"{(1 - len(table_status_df[table_status_df['table_status'] == 'Mismatch'])/len(table_status_df))*100:.2f}%" if len(table_status_df) > 0 else "0%")
    
    col4, col5, col6 = st.columns(3)
    col4.metric("ì´ ì»¬ëŸ¼ ìˆ˜", total_count)
    col5.metric("ì •í•©ì„± ë¶ˆì¼ì¹˜ ì»¬ëŸ¼ ìˆ˜", error_count)
    col6.metric("ì •í•©ì„± ì¼ì¹˜ìœ¨(ì»¬ëŸ¼ ê¸°ì¤€)", f"{(1 - error_count/total_count)*100:.2f}%" if total_count > 0 else "0%")

    st.markdown("---")
    
    st.dataframe(table_status_df.sort_values(by=['table_status', 'mismatch_columns'], ascending=[False, False]), 
                 use_container_width=True, height=300)

    st.markdown("---")
    st.markdown("#### ì •í•©ì„± ì²´í¬ ìƒì„¸ ì§€í‘œ")
    
    search_query = st.text_input(
        "í…Œì´ë¸”ëª… ê²€ìƒ‰",
        help="ì°¾ìœ¼ë ¤ëŠ” í…Œì´ë¸”ëª…ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: sales_table)",
        key=f"table_search_{src_schema_name}_{tgt_schema_name}"
    )

    status_options = merged_df['Status'].unique().tolist()
    selected_status = st.multiselect(
        "ìƒíƒœ í•„í„°",
        status_options,
        default=status_options,
        key=f"status_filter_{src_schema_name}_{tgt_schema_name}"
    )

    filtered_df = merged_df[
        merged_df['Status'].isin(selected_status) &
        (merged_df['table_name'].str.contains(search_query, case=False, na=False))
    ].sort_values(by=['Status', 'table_name', 'column_name'], ascending=[False, True, True])

    rows_per_page = 100
    total_rows = len(filtered_df)
    total_pages = (total_rows // rows_per_page) + (1 if total_rows % rows_per_page > 0 else 0)
    
    st.info(f"ì´ `{total_rows}`ê°œì˜ ê²°ê³¼ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    if total_pages > 1:
        page_number = st.number_input("í˜ì´ì§€ ë²ˆí˜¸", 1, total_pages, 1, key=f"page_{src_schema_name}_{tgt_schema_name}")
    else:
        page_number = 1

    start_row = (page_number - 1) * rows_per_page
    end_row = start_row + rows_per_page
    paginated_df = filtered_df.iloc[start_row:end_row]

    def highlight_status(row):
        if row['Status'] == 'OK':
            return ['background-color: #e0ffe0'] * len(row)
        else:
            return ['background-color: #ffcccc'] * len(row)
    
    # ğŸ’¡ 'Status_detail' ì»¬ëŸ¼ì„ ì¶”ê°€
    st.dataframe(
        paginated_df[[
            'table_schema', 'table_name', 'column_name', 'Status', 'Status_detail',
            'column_comment_src', 'column_comment_tgt',
            'data_type_src', 'data_type_tgt',
            'character_maximum_length_src', 'character_maximum_length_tgt',
            'numeric_precision_src', 'numeric_precision_tgt',
            'numeric_precision_radix_src', 'numeric_precision_radix_tgt'
        ]].style.apply(highlight_status, axis=1),
        use_container_width=True,
        hide_index=True,
        height=500
    )
    st.markdown("---")


# --------------------
# 4. TGT ê²€ì‚¬ ì „ìš© í•¨ìˆ˜
# --------------------
def check_tgt_comments(df):
    st.subheader("TGT ìŠ¤í‚¤ë§ˆ - ì½”ë©˜íŠ¸ ëˆ„ë½ ì»¬ëŸ¼ ê²€ì‚¬")
    
    st.markdown("""
    ì´ ê²€ì‚¬ëŠ” **ìš´ì˜ TOBE ìŠ¤í‚¤ë§ˆì˜ ì»¬ëŸ¼ ì¤‘ ì½”ë©˜íŠ¸ê°€ ëˆ„ë½ëœ ëª©ë¡**ì„ í™•ì¸í•©ë‹ˆë‹¤.
    - **ì ê²€ ê¸°ì¤€:** `column_comment`ê°€ `NULL`ì¸ ê²½ìš°
    - **ì½”ë©˜íŠ¸ ëˆ„ë½ ì›ì¸:**
      - ë°ì´í„° ëª¨ë¸ë§ ì‹œ ì½”ë©˜íŠ¸ê°€ ì‘ì„±ë˜ì§€ ì•ŠìŒ
      - ETL ì‘ì—… ì¤‘ ë©”íƒ€ë°ì´í„°ê°€ ëˆ„ë½ë¨
    """)
    
    no_comment_df = df[pd.isna(df['column_comment'])].sort_values(by=['tbl_schema', 'table_name', 'column_name'])
    
    total_count = len(df)
    no_comment_count = len(no_comment_df)
    
    st.metric("ì½”ë©˜íŠ¸ ëˆ„ë½ ì»¬ëŸ¼ ìˆ˜", no_comment_count)
    st.info(f"ì´ `{no_comment_count}`ê°œì˜ ì½”ë©˜íŠ¸ ëˆ„ë½ ì»¬ëŸ¼ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    st.dataframe(no_comment_df[[
        'tbl_schema', 'table_name', 'column_name', 'column_comment'
    ]], use_container_width=True, hide_index=True, height=500)


# --------------------
# 5. ì •í•©ì„± ì²´í¬ ì‹¤í–‰
# --------------------
with st.spinner('í…Œì´ë¸” ì •ë³´ë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...'):
    start_time = time.time()
    
    # DB ì ‘ì† ì •ë³´
    redshift_dev = {
        "host": st.secrets["redshift"]["dev_host"], "port": st.secrets["redshift"]["dev_port"],
        "user": st.secrets["redshift"]["dev_user"], "password": st.secrets["redshift"]["dev_password"],
        "database": st.secrets["redshift"]["dev_database"]
    }
    redshift_prod = {
        "host": st.secrets["redshift"]["prd_host"], "port": st.secrets["redshift"]["prd_port"],
        "user": st.secrets["redshift"]["prd_user"], "password": st.secrets["redshift"]["prd_password"],
        "database": st.secrets["redshift"]["prd_database"]
    }
    
    dev_tables_df = get_table_info(redshift_dev, ['gsbi_own', 'sdhub_own', 'dhub_own'])
    prod_tobe_tables_df = get_table_info(redshift_prod, ['gsbi_own', 'sdhub_own', 'dhub_own'])
    # prod_base_tables_df = get_table_info(redshift_prod, ['base_gsbi_own', 'base_sdhub_own', 'base_dhub_own'])

    dev_tables_df_filtered = dev_tables_df[dev_tables_df['tbl_schema'].isin(['gsbi_own', 'sdhub_own', 'dhub_own'])].copy()
    prod_tobe_tables_df_filtered = prod_tobe_tables_df[prod_tobe_tables_df['tbl_schema'].isin(['gsbi_own', 'sdhub_own', 'dhub_own'])].copy()
    # prod_base_tables_df_filtered = prod_base_tables_df[prod_base_tables_df['tbl_schema'].isin(['base_gsbi_own', 'base_sdhub_own', 'base_dhub_own'])].copy()

    tab1, tab2 = st.tabs(["ì •í•©ì„± ì²´í¬ (SRC/TGT)", "TGT ê²€ì‚¬"])

    with tab1:
        check_consistency(dev_tables_df_filtered, prod_tobe_tables_df_filtered, "ê°œë°œ TOBE", "ìš´ì˜ TOBE")
        # st.markdown("---")
        # check_consistency(prod_base_tables_df_filtered, prod_tobe_tables_df_filtered, "ìš´ì˜ BASE", "ìš´ì˜ TOBE")

    with tab2:
        check_tgt_comments(prod_tobe_tables_df_filtered)

    end_time = time.time()
    st.sidebar.success(f"ë°ì´í„° ë¡œë”© ë° ì •í•©ì„± ì²´í¬ ì™„ë£Œ\n(ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")
